   check_itemscale   Describe Properties of Item Scales
Description
 Compute various measures of internal consistencies applied to (sub)scales, which items were ex-
 tracted using parameters::principal_components().

#### Usage 
<pre><code>

 check_itemscale(x)
Arguments
 xAn object of class parameters_pca, as returned by parameters::principal_components().
Details
 check_itemscale() calculates various measures of internal consistencies, such as Cronbach’s al-
 pha, item difficulty or discrimination etc. on subscales which were built from several items. Sub-
 scales are retrieved from the results of parameters::principal_components(), i.e. based on how
 many components were extracted from the PCA, check_itemscale() retrieves those variables that
 belong to a component and calculates the above mentioned measures.
Value
 A list of data frames, with related measures of internal consistencies of each subscale.
Note
*  Item difficulty should range between 0.2 and 0.8. Ideal value is p+(1-p)/2 (which mostly is
   between 0.5 and 0.8). See item_difficulty for details.
*  For item discrimination, acceptable values are 0.20 or higher; the closer to 1.00 the better. See
   item_reliability for more details.
*  In case the total Cronbach’s alpha value is below the acceptable cut-off of 0.7 (mostly if an
   index has few items), the mean inter-item-correlation is an alternative measure to indicate
   acceptability. Satisfactory range lies between 0.2 and 0.4. See also item_intercor.

check_model13

#### Examples
# data generation from '?prcomp', slightly modified
C <- chol(S <- toeplitz(.9^(0:15)))
set.seed(17)
X <- matrix(rnorm(16000), 100, 16)
Z <- X %*% C
if (require("parameters") && require("psych")) {
  pca <- principal_components(as.data.frame(Z), rotation = "varimax", n = 3)
  pca
  check_itemscale(pca)
}
  check_model  Visual check of model assumptions
Description
Visual check of model various assumptions (normality of residuals, normality of random effects,
heteroscedasticity, homogeneity of variance, multicollinearity).

#### Usage 
<pre><code>

check_model(x, ...)
## Default S3 method:
check_model(
  x,
  dot_size = 2,
  line_size = 0.8,
  panel = TRUE,
  check = "all",
  alpha = 0.2,
  ...
)
Arguments
x   A model object.
... Currently not used.
dot_sizeSize of dot-geoms.
line_size   Size of line-geoms.
panel   Logical, if TRUE, plots are arranged as panels; else, single plots for each diag-
nostic are returned.
14  check_normality
 checkCharacter vector, indicating which checks for should be performed and plotted.
  May be one or more of "all","vif","qq","normality","ncv","homogeneity","outliers","reqq
  "reqq" is a QQ-plot for random effects and only available for mixed models.
  "ncv" checks for non-constant variance, i.e. for heteroscedasticity. By default,
  all possible checks are performed and plotted.
 alphaThe alpha level of the confidence bands. Scalar from 0 to 1.
Value
 The data frame that is used for plotting.
Note
 This function just prepares the data for plotting. To create the plots, see needs to be installed.
 Furthermore, this function suppresses all possible warnings. In case you observe suspicious plots,
 please refer to the dedicated functions (like check_collinearity(), check_normality() etc.) to
 get informative messages and warnings.

#### Examples
 ## Not run:
 m <- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
 check_model(m)
 if (require("lme4")) {
   m <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
   check_model(m, panel = FALSE)
 }
 if (require("rstanarm")) {
   m <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200)
   check_model(m)
 }
 ## End(Not run)
   check_normalityCheck model for (non-)normality of residuals.
Description
 Check model for (non-)normality of residuals.

#### Usage 
<pre><code>

 check_normality(x, ...)
 ## S3 method for class 'merMod'
 check_normality(x, effects = c("fixed", "random"), ...)
check_normality   15
Arguments
x   A model object.
... Currently not used.
effects Should normality for residuals ("fixed") or random effects ("random") be tested?
Only applies to mixed models. May be abbreviated.
Details
check_normality() calls stats::shapiro.test and checks the standardized residuals (or stu-
dentized residuals for mixed models) for normal distribution. Note that this formal test almost
always yields significant results for the distribution of residuals and visual inspection (e.g. Q-Q
plots) are preferable.
Value
Invisibly returns the p-value of the test statistics. A p-value < 0.05 indicates a significant deviation
from normal distribution
Note
For mixed models, studentized residuals are used for the test, not the standardized residuals. There
is also a plot()-method implemented in the see-package.

#### Examples
m <- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_normality(m)
# plot results
if (require("see")) {
   x <- check_normality(m)
   plot(x)
}
## Not run:
# QQ-plot
plot(check_normality(m), type = "qq")
# PP-plot
plot(check_normality(m), type = "pp")
## End(Not run)
16 check_outliers
   check_outliers Outliers detection (check for influential observations)
Description
 Checks for and locates influential observations (i.e., "outliers") via several distance and/or clustering
 methods. If several methods are selected, the returned "Outlier" vector will be a composite outlier
 score, made of the average of the binary (0 or 1) results of each method. It represents the probability
 of each observation of being classified as an outlier by at least one method. The decision rule used
 by default is to classify as outliers observations which composite outlier score is superior or equal
 to 0.5 (i.e., that were classified as outliers by at least half of the methods). See the Details section
 below for a description of the methods.

#### Usage 
<pre><code>

 check_outliers(x, ...)
 ## Default S3 method:
 check_outliers(x, method = c("cook", "pareto"), threshold = NULL, ...)
 ## S3 method for class 'numeric'
 check_outliers(x, method = "zscore", threshold = NULL, ...)
 ## S3 method for class 'data.frame'
 check_outliers(x, method = "mahalanobis", threshold = NULL, ...)
Arguments
 xA model or a data.frame object.
 ...  When method = "ics", further arguments in ... are passed down to ICSOutlier::ics.outlier().
 method   The outlier detection method(s). Can be "all" or some of c("cook", "pareto",
  "zscore", "iqr", "mahalanobis", "robust", "mcd", "ics", "optics", "lof").
 thresholdA list containing the threshold values for each method (e.g. list('mahalanobis'
  = 7,'cook' = 1)), above which an observation is considered as outlier. If NULL,
  default values will be used (see ’Details’).
Details
 Outliers can be defined as particularly influential observations. Most methods rely on the compu-
 tation of some distance metric, and the observations greater than a certain threshold are considered
 outliers. Importantly, outliers detection methods are meant to provide information to the researcher,
 rather than being an automatized procedure which mindless application is a substitute for thinking.
 An example sentence for reporting the 
#### Usage 
<pre><code>
 of the composite method could be:
 "Based on a composite outlier score (see the ’check_outliers’ function in the ’performance’ R pack-
 age; Lüdecke et al., 2019) obtained via the joint application of multiple outliers detection algo-
 rithms (Z-scores, Iglewicz, 1993; Interquartile range (IQR); Mahalanobis distance, Cabana, 2019;
check_outliers 17
Robust Mahalanobis distance, Gnanadesikan & Kettenring, 1972; Minimum Covariance Determi-
nant, Leys et al., 2018; Invariant Coordinate Selection, Archimbaud et al., 2018; OPTICS, Ankerst
et al., 1999; Isolation Forest, Liu et al. 2008; and Local Outlier Factor, Breunig et al., 2000), we
excluded n participants that were classified as outliers by at least half of the methods used."
  Model-specific methods:
*  Cook’s Distance: Among outlier detection methods, Cook’s distance and leverage are less
common than the basic Mahalanobis distance, but still used. Cook’s distance estimates the
variations in regression coefficients after removing each observation, one by one (Cook,
1977). Since Cook’s distance is in the metric of an F distribution with p and n-p degrees
of freedom, the median point of the quantile distribution can be used as a cut-off (Bollen,
1985). A common approximation or heuristic is to use 4 divided by the numbers of observa-
tions, which usually corresponds to a lower threshold (i.e., more outliers are detected). This
only works for Frequentist models. For Bayesian models, see pareto.
*  Pareto: The reliability and approximate convergence of Bayesian models can be assessed
using the estimates for the shape parameter k of the generalized Pareto distribution. If the
estimated tail shape parameter k exceeds 0.5, the user should be warned, although in practice
the authors of the loo package observed good performance for values of k up to 0.7 (the
default threshold used by performance).
  Univariate methods:
*  Z-scores: The Z-score, or standard score, is a way of describing a data point as deviance
from a central value, in terms of standard deviations from the mean or, as it is here the case
by default (Iglewicz, 1993), in terms of Median Absolute Deviation (MAD) from the me-
dian (which are robust measures of dispersion and centrality). The default threshold to clas-
sify outliers is 1.959 (threshold = list("zscore" = = 1.959)), corresponding to the 2.5%
(qnorm(0.975)) most extreme observations (assuming the data is normally distributed). Im-
portantly, the Z-score method is univariate: it is computed column by column. If a data.frame
is passed, then the maximum distance is kept for each observations. Thus, all observations
that are extreme for at least one variable will be detected. However, this method is not suited
for high dimensional data (with many columns), returning too liberal results (detecting many
outliers).
*  IQR: Using the IQR (interquartile range) is a robust method developed by John Tukey, which
often appears in box-and-whisker plots (e.g., in geom_boxplot). The interquartile range is
the range between the first and the third quartiles. Tukey considered as outliers any data point
that fell outside of either 1.5 times (the default threshold) the IQR below the first or above
the third quartile. Similar to the Z-score method, this is a univariate method for outliers
detection, returning outliers detected for at least one column, and might thus not be suited to
high dimensional data.
  Multivariate methods:
*  Mahalanobis Distance: Mahalanobis distance (Mahalanobis, 1930) is often used for multi-
variate outliers detection as this distance takes into account the shape of the observations. The
default threshold is often arbitrarily set to some deviation (in terms of SD or MAD) from
the mean (or median) of the Mahalanobis distance. However, as the Mahalanobis distance
can be approximated by a Chi squared distribution (Rousseeuw & Van Zomeren, 1990), we
can use the alpha quantile of the chi-square distribution with k degrees of freedom (k being
the number of columns). By default, the alpha threshold is set to 0.025 (corresponding to the
18   check_outliers
   2.5% most extreme observations; Cabana, 2019). This criterion is a natural extension of the
   median plus or minus a coefficient times the MAD method (Leys et al., 2013).
*  Robust Mahalanobis Distance: A robust version of Mahalanobis distance using an Orthog-
   onalized Gnanadesikan-Kettenring pairwise estimator (Gnanadesikan \& Kettenring, 1972).
   Requires the bigutilsr package.
*  Minimum Covariance Determinant (MCD): Another robust version of Mahalanobis. Leys
   et al. (2018) argue that Mahalanobis Distance is not a robust way to determine outliers, as it
   uses the means and covariances of all the data – including the outliers – to determine individ-
   ual difference scores. Minimum Covariance Determinant calculates the mean and covariance
   matrix based on the most central subset of the data (by default, 66%), before computing
   the Mahalanobis Distance. This is deemed to be a more robust method of identifying and
   removing outliers than regular Mahalanobis distance.
*  Invariant Coordinate Selection (ICS): The outlier are detected using ICS, which by default
   uses an alpha threshold of 0.025 (corresponding to the 2.5% most extreme observations) as a
   cut-off value for outliers classification. Refer to the help-file of ICSOutlier::ics.outlier()
   to get more details about this procedure. Note that method = "ics" requires both ICS and
   ICSOutlier to be installed, and that it takes some time to compute the results.
*  OPTICS: The Ordering Points To Identify the Clustering Structure (OPTICS) algorithm
   (Ankerst et al., 1999) is using similar concepts to DBSCAN (an unsupervised clustering tech-
   nique that can be used for outliers detection). The threshold argument is passed as minPts,
   which corresponds to the minimum size of a cluster. By default, this size is set at 2 times
   the number of columns (Sander et al., 1998). Compared to the others techniques, that will
   always detect several outliers (as these are usually defined as a percentage of extreme val-
   ues), this algorithm functions in a different manner and won’t always detect outliers. Note
   that method = "optics" requires the dbscan package to be installed, and that it takes some
   time to compute the results.
*  Isolation Forest: The outliers are detected using the anomaly score of an isolation forest (a
   class of random forest). The default threshold of 0.025 will classify as outliers the observa-
   tions located at qnorm(1-0.025) * MAD) (a robust equivalent of SD) of the median (roughly
   corresponding to the 2.5% most extreme observations). Requires the solitude package.
*  Local Outlier Factor: Based on a K nearest neighbours algorithm, LOF compares the local
   density of an point to the local densities of its neighbors instead of computing a distance from
   the center (Breunig et al., 2000). Points that have a substantially lower density than their
   neighbors are considered outliers. A LOF score of approximately 1 indicates that density
   around the point is comparable to its neighbors. Scores significantly larger than 1 indicate
   outliers. The default threshold of 0.025 will classify as outliers the observations located at
   qnorm(1-0.025) * SD) of the log-transformed LOF distance. Requires the dbscan package.
  Threshold specification: Default thresholds are currently specified as follows:
   list( zscore = stats::qnorm(p = 1 -0.025),iqr = 1.5,cook = stats::qf(0.5,ncol(x),nrow(x)
  -ncol(x)),pareto = 0.7,mahalanobis = stats::qchisq(p = 1 -0.025,df = ncol(x)),robust
  = stats::qchisq(p = 1 -0.025,df = ncol(x)),mcd = stats::qchisq(p = 1 -0.025,df = ncol(x)),ics
  = 0.025,optics = 2 * ncol(x),iforest = 0.025,lof = 0.025 )
Value
A logical vector of the detected outliers with a nice printing method: a check (message) on whether
outliers were detected or not. The information on the distance measure and whether or not an
observation is considered as outlier can be recovered with the as.data.frame function.
check_outliers  19
Note
There is also a plot()-method implemented in the see-package.


#### Examples
# Univariate
check_outliers(mtcars$mpg)
# Multivariate
# select only mpg and disp (continuous)
mt1 <- mtcars[, c(1, 3, 4)]
# create some fake outliers and attach outliers to main df
mt2 <- rbind(mt1, data.frame(mpg = c(37, 40), disp = c(300, 400), hp = c(110, 120)))
# fit model with outliers
model <- lm(disp ~ mpg + hp, data = mt2)
ol <- check_outliers(model)
# plot(ol)
insight::get_data(model)[ol, ]
if (require("MASS")) {
  check_outliers(model, method = c("mahalabonis", "mcd"))
}
## Not run:
# This one takes some seconds to finish...
check_outliers(model, method = "ics")
20   check_overdispersion
 # For dataframes
 check_outliers(mtcars)
 check_outliers(mtcars, method = "all")
 ## End(Not run)

	 


